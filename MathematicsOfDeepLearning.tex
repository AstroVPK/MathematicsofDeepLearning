%%
%% Beginning of file 'sample61.tex'
%%
%% Modified 2016 September
%%
%% This is a sample manuscript marked up using the
%% AASTeX v6.1 LaTeX 2e macros.
%%
%% AASTeX is now based on Alexey Vikhlinin's emulateapj.cls
%% (Copyright 2000-2015).  See the classfile for details.

%% AASTeX requires revtex4-1.cls (http://publish.aps.org/revtex4/) and
%% other external packages (latexsym, graphicx, amssymb, longtable, and epsf).
%% All of these external packages should already be present in the modern TeX
%% distributions.  If not they can also be obtained at www.ctan.org.

%% The first piece of markup in an AASTeX v6.x document is the \documentclass
%% command. LaTeX will ignore any data that comes before this command. The
%% documentclass can take an optional argument to modify the output style.
%% The command below calls the preprint style  which will produce a tightly
%% typeset, one-column, single-spaced document.  It is the default and thus
%% does not need to be explicitly stated.
%%
%%
%% using aastex version 6.1
\documentclass[modern]{aastex61}

%% The default is a single spaced, 10 point font, single spaced article.
%% There are 5 other style options available via an optional argument. They
%% can be envoked like this:
%%
%% \documentclass[argument]{aastex61}
%%
%% where the arguement options are:
%%
%%  twocolumn   : two text columns, 10 point font, single spaced article.
%%                This is the most compact and represent the final published
%%                derived PDF copy of the accepted manuscript from the publisher
%%  manuscript  : one text column, 12 point font, double spaced article.
%%  preprint    : one text column, 12 point font, single spaced article.
%%  preprint2   : two text columns, 12 point font, single spaced article.
%%  modern      : a stylish, single text column, 12 point font, article with
%% 		  wider left and right margins. This uses the Daniel
%% 		  Foreman-Mackey and David Hogg design.
%%
%% Note that you can submit to the AAS Journals in any of these 6 styles.
%%
%% There are other optional arguments one can envoke to allow other stylistic
%% actions. The available options are:
%%
%%  astrosymb    : Loads Astrosymb font and define \astrocommands.
%%  tighten      : Makes baselineskip slightly smaller, only works with
%%                 the twocolumn substyle.
%%  times        : uses times font instead of the default
%%  linenumbers  : turn on lineno package.
%%  trackchanges : required to see the revision mark up and print its output
%%  longauthor   : Do not use the more compressed footnote style (default) for
%%                 the author/collaboration/affiliations. Instead print all
%%                 affiliation information after each name. Creates a much
%%                 long author list but may be desirable for short author papers
%%
%% these can be used in any combination, e.g.
%%
%% \documentclass[twocolumn,linenumbers,trackchanges]{aastex61}

%% AASTeX v6.* now includes \hyperref support. While we have built in specific
%% defaults into the classfile you can manually override them with the
%% \hypersetup command. For example,
%%
%%\hypersetup{linkcolor=red,citecolor=green,filecolor=cyan,urlcolor=magenta}
%%
%% will change the color of the internal links to red, the links to the
%% bibliography to green, the file links to cyan, and the external links to
%% magenta. Additional information on \hyperref options can be found here:
%% https://www.tug.org/applications/hyperref/manual.html#x1-40003

%% If you want to create your own macros, you can do so
%% using \newcommand. Your macros should appear before
%% the \begin{document} command.
%%
\usepackage{mathrsfs}
\usepackage{hyperref}	% Hyperlinks
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,filecolor=blue,urlcolor=blue}
\usepackage{newtxtext,newtxmath}
\newcommand{\vdag}{(v)^\dagger}
\newcommand\aastex{AAS\TeX}
\newcommand\latex{La\TeX}
\newcommand{\ue}{\mathrm{e}}
\newcommand{\uderivative}{\mathrm{d}}
\newcommand{\un}{\mathrm{n}}
\input{customizations.tex}

%% Reintroduced the \received and \accepted commands from AASTeX v5.2
\received{December 28, 2017}
\revised{December 28, 2017}
\accepted{\today}
%% Command to document which AAS Journal the manuscript was submitted to.
%% Adds "Submitted to " the arguement.
%\submitjournal{ApJ}

%% Mark up commands to limit the number of authors on the front page.
%% Note that in AASTeX v6.1 a \collaboration call (see below) counts as
%% an author in this case.
%
%\AuthorCollaborationLimit=3
%
%% Will only show Schwarz, Muench and "the AAS Journals Data Scientist
%% collaboration" on the front page of this example manuscript.
%%
%% Note that all of the author will be shown in the published article.
%% This feature is meant to be used prior to acceptance to make the
%% front end of a long author article more manageable. Please do not use
%% this functionality for manuscripts with less than 20 authors. Conversely,
%% please do use this when the number of authors exceeds 40.
%%
%% Use \allauthors at the manuscript end to show the full author list.
%% This command should only be used with \AuthorCollaborationLimit is used.

%% The following command can be used to set the latex table counters.  It
%% is needed in this document because it uses a mix of latex tabular and
%% AASTeX deluxetables.  In general it should not be needed.
%\setcounter{table}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% The following section outlines numerous optional output that
%% can be displayed in the front matter or as running meta-data.
%%
%% If you wish, you may supply running head information, although
%% this information may be modified by the editorial offices.
\shorttitle{Deep Learning Math}
\shortauthors{Kasliwal, 2017}
%%
%% You can add a light gray and diagonal water-mark to the first page
%% with this command:
% \watermark{text}
%% where "text", e.g. DRAFT, is the text to appear.  If the text is
%% long you can control the water-mark size with:
%  \setwatermarkfontsize{dimension}
%% where dimension is any recognized LaTeX dimension, e.g. pt, in, etc.
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% This is the end of the preamble.  Indicate the beginning of the
%% manuscript itself with \begin{document}.

\begin{document}

\title{The Mathematics of Deep Learning\footnote{Draft prepared for personal use.}}

%% LaTeX will automatically break titles if they run longer than
%% one line. However, you may use \\ to force a line break if
%% you desire. In v6.1 you can include a footnote in the title.

%% A significant change from earlier AASTEX versions is in the structure for
%% calling author and affilations. The change was necessary to implement
%% autoindexing of affilations which prior was a manual process that could
%% easily be tedious in large author manuscripts.
%%
%% The \author command is the same as before except it now takes an optional
%% arguement which is the 16 digit ORCID. The syntax is:
%% \author[xxxx-xxxx-xxxx-xxxx]{Author Name}
%%
%% This will hyperlink the author name to the author's ORCID page. Note that
%% during compilation, LaTeX will do some limited checking of the format of
%% the ID to make sure it is valid.
%%
%% Use \affiliation for affiliation information. The old \affil is now aliased
%% to \affiliation. AASTeX v6.1 will automatically index these in the header.
%% When a duplicate is found its index will be the same as its previous entry.
%%
%% Note that \altaffilmark and \altaffiltext have been removed and thus
%% can not be used to document secondary affiliations. If they are used latex
%% will issue a specific error message and quit. Please use multiple
%% \affiliation calls for to document more than one affiliation.
%%
%% The new \altaffiliation can be used to indicate some secondary information
%% such as fellowships. This command produces a non-numeric footnote that is
%% set away from the numeric \affiliation footnotes.  NOTE that if an
%% \altaffiliation command is used it must come BEFORE the \affiliation call,
%% right after the \author command, in order to place the footnotes in
%% the proper location.
%%
%% Use \email to set provide email addresses. Each \email will appear on its
%% own line so you can put multiple email address in one \email call. A new
%% \correspondingauthor command is available in V6.1 to identify the
%% corresponding author of the manuscript. It is the author's responsibility
%% to make sure this name is also in the author list.
%%
%% While authors can be grouped inside the same \author and \affiliation
%% commands it is better to have a single author for each. This allows for
%% one to exploit all the new benefits and should make book-keeping easier.
%%
%% If done correctly the peer review system will be able to
%% automatically put the author and affiliation information from the manuscript
%% and save the corresponding author the trouble of entering it by hand.

\correspondingauthor{Vishal Kasliwal}
\email{vishal@wavecomp.com, vishal.kasliwal@gmail.com}

\author[0000-0001-7970-0760]{Vishal P. Kasliwal}
\affil{Wave Computing \\
42 W. Campbell Ave, \# 301 \\
Campbell, CA 95008, USA}

%% Note that the \and command from previous versions of AASTeX is now
%% depreciated in this version as it is no longer necessary. AASTeX
%% automatically takes care of all commas and "and"s between authors names.

%% AASTeX 6.1 has the new \collaboration and \nocollaboration commands to
%% provide the collaboration status of a group of authors. These commands
%% can be used either before or after the list of corresponding authors. The
%% argument for \collaboration is the collaboration identifier. Authors are
%% encouraged to surround collaboration identifiers with ()s. The
%% \nocollaboration command takes no argument and exists to indicate that
%% the nearby authors are not part of surrounding collaborations.

%% Mark off the abstract in the ``abstract'' environment.
\begin{abstract}

Deep Learning is a branch of Machine Learning in which `deep' neural networks are used for various purposes.

\end{abstract}

%% Keywords should appear after the \end{abstract} command.
%% See the online documentation for the full list of available subject
%% keywords and the rules for their use.
\keywords{neural networks, training}

%% From the front matter, we move on to the body of the paper.
%% Sections are demarcated by \section and \subsection, respectively.
%% Observe the use of the LaTeX \label
%% command after the \subsection to give a symbolic KEY to the
%% subsection for cross-referencing in a \ref command.
%% You can use LaTeX's \ref and \label commands to keep track of
%% cross-references to sections, equations, tables, and figures.
%% That way, if you change the order of any elements, LaTeX will
%% automatically renumber them.

%% We recommend that authors also use the natbib \citep
%% and \citet commands to identify citations.  The citations are
%% tied to the reference list via symbolic KEYs. The KEY corresponds
%% to the KEY in the \bibitem in the reference list below.

\section{Preliminaries} \label{sec:prelim}

\section{Notation} \label{sec:notation}

%We shall use \mathbfit{a} to denote vectors and \mathbfss{A} to denote tensors.

Tensors are associated with layers. The $p \times q$ tensor $\mathbfss{A}$ when associated with layer $l$ is denoted $\mathbfss{A}_{p \times q}^{[l]}$. The transpose of this tensor is denoted by $(\mathbfss{A}_{p \times q}^{[l]})^{\top}$. If the tensor is an input tensor to a layer, it may additionally have a minibatch index, $m$, and instance within the minibatch, $i$, associated with it. Thus $(\mathbfss{A}_{p \times q}^{[l]\{m\}(i)})^{\top}$ is the transpose of the $i$-th training example from the $m$-th minibatch in the $l$-th layer from the input tensor $\mathbfss{A}$ which is $p$ rows high by $q$ columns long. Furthermore, layers associated with convolutional neural networks may have \textit{channels} associated with them. Channels add extra dimensions to tensors and so $(\mathbfss{A}_{p \times q \times k_{1} \times k_{2}}^{[l]\{m\}(i)})^{\top}$ is the transpose of the $i$-th training example from the $m$-th minibatch in the $l$-th layer from the input tensor $\mathbfss{A}$ which is $p$ rows high by $q$ columns long and has two sets of channels of depth $k_{1}$ and $k_{2}$ respectively.

The $\un^{[l]}$ operator returns the number of nodes, i.e. the width, of the $l$-th layer. Traditionally, the depth of the neural network is denoted by $L$, and hence we must have $0 \leq l \leq L$.

We transform the $l-1$-th layer of activations $\mathbfss{A}_{p \times q}^{[l-1]\{m\}(i)}$, using neural network operations such as convolution etc... in the $l$-th layer to produce the $l$-th layer of activations i.e. $\mathbfss{A}_{p \times q}^{[l]\{m\}(i)}$.

Neural networks use several different types of products. We shall denote the standard scalar product of two numbers with \textit{no} special symbol i.e. the product of the scalars $a$ \& $b$ shall be denoted by $ab$.

Note that we distinguish between the tensor $\mathbfss{T}$ and the $i$-, $j$-th component of that tensor $T_{ij}$. The tensors encountered in Deep Learning can have lots of large dimensions. Hence, we shall write all operations using components as opposed to whole tensors. There is no downside to this choice since we never perform coordinate transformations in Deep Learning and hance writing tensor equations buys us no benefit.

\section{Operations} \label{sec:op}


\subsection{ReLU Activation Function} \label{sec:relu}

The ReLU (Rectified Linear Unit) activation function takes the form
\begin{equation} \label{eq:relu}
  g(z) =
  \begin{cases}
    0 & \text{if $z < 0$} \\
    z & \text{otherwise},
  \end{cases}
\end{equation}
making it discontinuous at $z = 0$. Deep Learning cheats around this discontinuity by defining the derivative so that it is either right-continuous or left-continuous. In this work, we shall choose to make the derivative right-continuous. Therefore, the derivative of the ReLU activation function is
\begin{equation} \label{eq:gradrelu}
  \frac{\uderivative g(z)}{\uderivative z} =
  \begin{cases}
    0 & \text{if $z < 0$} \\
    1 & \text{otherwise},
  \end{cases}
\end{equation}

\subsubsection{Derivation}

It is evident that for $z < 0$,
\begin{equation*}
  \frac{\uderivative g(z)}{\uderivative z} = 0,
\end{equation*}
since $g(z) = 0 \forall z < 0$. Similarly, since $g(z) = 1 \forall z \geq 0$,
\begin{equation*}
  \frac{\uderivative g(z)}{\uderivative z} = 1,
\end{equation*}
$\forall z \geq 0$. Since
\begin{equation*}
  \underset{z \uparrow 0}{\lim}\frac{\uderivative g(z)}{\uderivative z} = 0,
\end{equation*}
and
\begin{equation*}
  \underset{z \downarrow 0}{\lim}\frac{\uderivative g(z)}{\uderivative z} = 1,
\end{equation*}
we can arbitarily choose to define
\begin{equation*}
  \frac{\uderivative g(z)}{\uderivative z}\Bigr|_{z = 0} = 1,
\end{equation*}

\subsection{Sigmoid Activation Function} \label{sec:sigmoid}

The sigmoid activation function takes the form
\begin{equation} \label{eq:sigmoid}
  g(z) = \sigma(z) = \frac{1}{1 + \ue^{-z}}.
\end{equation}
The derivative of the sigmoid activation function is
\begin{equation} \label{eq:sigmoidderivative}
  \frac{\uderivative \sigma}{\uderivative z} = \sigma(1 - \sigma).
\end{equation}

\subsubsection{Derivation}

By definition
\begin{equation*}
  \sigma(z) = \frac{1}{1 + \ue^{-z}} = (1 + \ue^{-z})^{-1},
\end{equation*}
and so
\begin{equation*}
  \frac{\uderivative \sigma(z)}{\uderivative z} = -(1 + \ue^{-z})^{-2} \frac{\uderivative (1 + \ue^{-z}) }{\uderivative z}.
\end{equation*}
But this is just
\begin{equation*}
  -(1 + \ue^{-z})^{-2} (-\ue^{-z}) = \sigma^{2} \ue^{-z}.
\end{equation*}
Now $\ue^{-z} = \frac{1}{\sigma} - 1 = \frac{1 - \sigma}{\sigma}$ and so
\begin{equation*}
  \frac{\uderivative \sigma(z)}{\uderivative z} = \sigma^{2} \ue^{-z} = \sigma^{2} \frac{1 - \sigma}{\sigma} = \sigma (1 - \sigma),
\end{equation*}
Q.E.D.


\subsection{Bias (Convolutional Layer)} \label{sec:bias}

Convolutional layers usually include a bias term of the following form -
\begin{equation} \label{eq:convbias}
  \mathbfss{Z}^{(m)} = \mathbfss{Y}^{(m)} + \mathbfit{b},
\end{equation}
i.e.
\begin{equation} \label{eq:convbias}
  Z^{(m)}_{hwc} = Y^{(m)}_{hwc} + b_{c},
\end{equation}
where the same bias is applied to each pixel within the same channel i.e. equation \eqref{eq:convbias} expands to $H \times W$ equations by Einstein's summation convention. The derivative of the loss with respect to the bias terms are given by
\begin{equation} \label{eq:gradconvbias}
  \frac{\uderivative \mathscr{L}}{\uderivative b_{c}} = \sum_{a, i, j = 1}^{M, H, W} \frac{\partial \mathscr{L}}{\partial Z^{(a)}_{ijc}},
\end{equation}
while the derivative of the loss with respect to the inputs are given by
\begin{equation} \label{eq:propconvbias}
  \frac{\uderivative \mathscr{L}}{\uderivative Y^{(m)}_{hwc}} = \frac{\partial \mathscr{L}}{\partial  Z^{(m)}_{hwc}}
\end{equation}

\subsubsection{Derivation}
First, consider the derivative of the loss with respect to the bias. By the chain rule, we have
\begin{equation*}
  \frac{\uderivative \mathscr{L}}{\uderivative b_{c}} = \sum_{a, i, j, k = 1}^{M, H, W, C} \frac{\partial \mathscr{L}}{\partial Z^{(a)}_{ijk}} \frac{\uderivative Z^{(a)}_{ijk}}{\uderivative b_{c}}.
\end{equation*}
Notice that
\begin{equation*}
  \frac{\uderivative Z^{(a)}_{ijk}}{\uderivative b_{c}} = \delta_{kc},
\end{equation*} so that
\begin{equation*}
  \frac{\uderivative \mathscr{L}}{\uderivative b_{c}} = \sum_{a, i, j, k = 1}^{M, H, W, C} \delta_{ck} \frac{\partial \mathscr{L}}{\partial Z^{(a)}_{ijk}} = \sum_{a, i, j = 1}^{M, H, W} \frac{\partial \mathscr{L}}{\partial Z^{(a)}_{ijc}},
\end{equation*}
Q.E.D.

Now consider the derivative of the loss with respect to the input derivatives from the previous operation. Again, applying the chain rule we get
\begin{equation*}
  \frac{\uderivative \mathscr{L}}{\uderivative Y^{(m)}_{hwc}} = \sum_{a,i,j,k = 1}^{M,H,W,C} \frac{\partial \mathscr{L}}{\partial Z^{(a)}_{ijk}} \frac{\uderivative  Z^{(a)}_{ijk}}{\uderivative Y^{(m)}_{hwc}}.
\end{equation*}
But notice that
\begin{equation*}
  \frac{\uderivative Z^{(a)}_{ijk}}{\uderivative Y^{(m)}_{hwc}} = \delta_{am}\delta_{ih}\delta_{jw}\delta_{kc},
\end{equation*}
so that
\begin{equation*}
  \frac{\uderivative \mathscr{L}}{\uderivative Y^{(m)}_{hwc}} = \sum_{a,i,j,k = 1}^{M,H,W,C} \delta_{am}\delta_{ih}\delta_{jw}\delta_{kc} \frac{\partial \mathscr{L}}{\partial  Z^{(a)}_{ijk}} = \frac{\partial \mathscr{L}}{\partial  Z^{(m)}_{hwc}},
\end{equation*}
Q.E.D.

\subsection{Convolution} \label{sec:conv}

Convolution operations convolve a set of $C \times F \times F \times C'$-dimensional filters with a $H' \times W' \times C'_{\mathrm{i}}$-dimensional input tensor $\mathbfss{X}$ producing the $H \times W \times C$-dimensional output tensor $\mathbfss{Y}$ via
\begin{equation}\label{eq:convtensorial}
  \mathbfss{Y}^{(m)} = \mathbfss{W} \ast \mathbfss{X}^{(m)}.
\end{equation}
$\mathbfss{X}$ is often padded with $0$s in the $H$ \& $W$ dimensions in order to make the size of the tensor a multiple of the filter size. Padding $\mathbfss{X}$ (dimensions $H' \times W' \times C'$) with $p$ zeros produces the padded tensor $\tilde{\mathbfss{X}}$ (dimensions $\tilde{H}' \times \tilde{W}' \times C' \equiv H' + 2p \times W' + 2p \times C$). Additionally, the {\it stride } $s$ of the convolution is not always $1$. Given non-zero padding $p$ and non-unit stride $s$, the convolution operation in equation \eqref{eq:convtensorial} can be computed as
\begin{equation} \label{eq:conv}
  Y^{(m)}_{abc} = \sum^{\frac{F-1}{2},C'}_{f_{1},f_{2} = \frac{1-F}{2}, c' = 1} W_{cf_{1}f_{2}c'} \tilde{X}^{(m)}_{a'+f_{1}, b'+f_{2},c'},
\end{equation}
with $a' = \frac{F - 1}{2} + (a - 1)s$ and $b' = \frac{F - 1}{2} + (b - 1)s$.

The derivative of the loss with respect to the filter $\mathbfss{W}$ is given by
\begin{equation}
  \frac{\uderivative \mathscr{L}}{\uderivative W_{cf_{1}f_{2}c'}} = \sum_{mab}^{MHW} \tilde{X}^{(m)}_{a' + f_{1}, b' + f_{2}, c'} \frac{\partial L}{\partial Y^{(m)}_{abc}},
\end{equation}
with $a' = \frac{F - 1}{2} + (a - 1)s$ and $b' = \frac{F - 1}{2} + (b - 1)s$.

\subsubsection{Derivation}

We wish to compute the total derivative of the loss $\mathscr{L}$ with respect to the filter $\mathbfss{W}$ and also the total derivative of $\mathscr{L}$ with respect to the input tensor $\mathbfss{X}$ given the derivative of $\mathscr{L}$ with respect to the output tensor $\mathbfss{Y}$.

First lets compute the total derivative of the loss $\mathscr{L}$ with respect to the filter $\mathbfss{W}$. Using the chain rule, we have
\begin{equation*}
  \frac{\uderivative \mathscr{L}}{\uderivative W_{cf_{1}f_{2}c'}} = \sum_{mabk}^{MHWC} \frac{\partial \mathscr{L}}{\partial Y^{(m)}_{abk}} \frac{\uderivative Y^{(m)}_{abk}}{\uderivative W_{cf_{1}f_{2}c'}}.
\end{equation*}
Equation \eqref{eq:conv} gives
\begin{equation*}
  \frac{\uderivative Y^{(m)}_{abk}}{\uderivative W_{cf_{1}f_{2}c'}} = \delta_{kc} \tilde{X}^{(m)}_{a' + f_{1}, b' + f_{2}, c'},
\end{equation*}
with $a' = \frac{F - 1}{2} + (a - 1)s$ and $b' = \frac{F - 1}{2} + (b - 1)s$. So
\begin{equation*}
  \frac{\uderivative \mathscr{L}}{\uderivative W_{cf_{1}f_{2}c'}} = \sum_{mabk}^{MHWC} \delta_{kc} \tilde{X}^{(m)}_{a' + f_{1}, b' + f_{2}, c'} \frac{\partial L}{\partial Y^{(m)}_{abk}} = \sum_{mab}^{MHW} \tilde{X}^{(m)}_{a' + f_{1}, b' + f_{2}, c'} \frac{\partial L}{\partial Y^{(m)}_{abc}}
\end{equation*}

Similarly, we may compute the total derivative of $\mathscr{L}$ with respect to the inputs $\mathbfss{X}$ using the chain rule as follows
\begin{equation*}
  \frac{\uderivative \mathscr{L}}{\uderivative X^{(m)}_{a'b'c'}} = \sum_{lijk}^{MHWC} \frac{\partial \mathscr{L}}{\partial Y^{(l)}_{ijk}} \frac{\uderivative Y^{(l)}_{ijk}}{\uderivative X^{(m)}_{a'b'c'}}.
\end{equation*}
Now
\begin{equation*}
  \frac{\uderivative Y^{(l)}_{ijk}}{\uderivative X^{(m)}_{a'b'c'}} = \delta_{lm}
\end{equation*}

\section{Output Layers} \label{sec:out}

\section{Feed-Forward Layers} \label{sec:ff}

\section{Convolutional Layers} \label{sec:conv}

\subsection{$1 \times 1$ Convolution} \label{sec:conv11}

$1 \times 1$ convolution is a misnomer. Recall that both the input and output activation tensors may have more than $1$ channels. $1 \times 1$ convolution essentially convolves over all the channels in the input layer and thus there is an implicit hidden dimension in the term $1 \times 1$ convolution i.e. the reader should really understand the term $1 \times 1$ convolution to mean $1 \times 1 \times D$ convolution where $D$ is the depth of the input tensor.

By convention, the size of the filter in the $3^{\mathrm{rd}}$ dimension always matches the depth of the input layer. Visualize $1 \times 1$ convolution as follows: Imagine the input tensor to be a block of numbers with height, width, and depth equal to $H \times W \times D$. The $1 \times 1$ convolution filter is a pencil of numbers that is $1 \times 1 \times D$. Starting at the top right corner of the input tensor, scan the pencil across the height and width of the tensor while keeping the depth of the pencil aligned with the depth of the tensor. The output of $1 \times 1$ convolution is just the inner product of the pencil with the vector of numbers from the input tensor that the pencil overlaps with. It is readily apparent that this operation will produce an output tensor of the same height and width as the input (assuming a stride of $1$) with depth equal to $1$ i.e. a normal matrix.

It is rarely the case that a single $1 \times 1 \times \un^{[l-1]}$ convolution is applied to the input tensor. More often than not, multiple filters i.e. a \textit{filter bank} is applied to the input tensor. If the filter bank contains $C$ filters, the filter bank has dimensions $1 \times 1 \times D \times C$ and produces an output tensor of dimensions $H \times W \times C$ (again assuming unit stride).

Mathematically, $1 \times 1$ convolution performs the following operations--
\begin{align} \label{eq:conv11}
\begin{split}
  Z^{[l](m)}_{hwc} &= \sum_{i}^{D} W^{[l]}_{ic}A^{[l-1](m)}_{h^{'}w^{'}i} + b^{[l]}_{c},
\\
  A^{[l](m)}_{hwc} &= g^{[l]}(Z^{[l](m)}_{hwc}),
\end{split}
\end{align}
with $h^{'} = hs$ \& $w^{'} = ws$ where $s$ is the stride. Notice that since $b^{[l]}_{c} $ carries just a channel index, we have a single value of $b$ for each output channel. As is evident from the Einstein summation convention, equation \label{eq:conv11} is really a full set of equations, one per output pixel. In the \textsc{Python} programming language, we would say that we \textit{broadcast} $b$ to the shape of $Z^{[l](m)}_{hwc}$.

The derivatives of the loss function $\mathscr{L}$ for $1 \times 1 \times D$ convolution are given by
\begin{equation} \label{eq:conv11dW}
  \frac{\uderivative \mathscr{L}}{\uderivative W^{[l]}_{dc}} = \sum_{m,i,j}^{M,H,W} A^{[l](m)}_{i^{'}j^{'}d} \frac{\partial \mathscr{L}}{\partial Z^{[l](m)}_{ijc}},
\end{equation}
\begin{equation} \label{eq:conv11db}
  \frac{\uderivative \mathscr{L}}{\uderivative b^{[l]}_{c}} = \sum_{m,i,j}^{M,H,W} \frac{\partial \mathscr{L}}{\partial Z^{[l](m)}_{ijc}},
\end{equation}
and
\begin{equation} \label{eq:conv11dA_lMinus}
  \frac{\uderivative \mathscr{L}}{\uderivative A^{[l-1](m)}_{h^{'}w^{'}d}} = \sum_{i,j,k}^{H,W,C} W^{[l]}_{kd}  \frac{\partial \mathscr{L}}{\partial Z^{[l](m)}_{ijk}}
\end{equation}
with
\begin{equation} \label{eq:conv11dZ}
  \frac{\partial \mathscr{L}}{\partial Z^{[l](m)}_{ijk}} = \frac{\partial \mathscr{L}}{\partial A^{[l](m)}_{ijk}} \frac{\uderivative g^{[l]}(Z^{[l](m)}_{ijk})}{\uderivative Z^{[l](m)}_{ijk}}.
\end{equation}

\subsubsection{Derivation}

We want the total derivative of $\mathscr{L}$ with respect to a given $W^{[l]}_{dc}$ i.e. $\uderivative \mathscr{L}/\uderivative W^{[l]}_{dc}$. Using the chain rule, we may write the total derivative as
\begin{equation*}
  \frac{\uderivative \mathscr{L}}{\uderivative W^{[l]}_{dc}} = \sum_{m,i,j,k=1}^{M,H,W,C} \frac{\partial \mathscr{L}}{\partial Z^{[l](m)}_{ijk}} \frac{\uderivative Z^{[l](m)}_{ijk}}{\uderivative W^{[l]}_{dc}}.
\end{equation*}
Notice that
\begin{equation*}
  \frac{\uderivative Z^{[l](m)}_{ijk}}{\uderivative W^{[l]}_{dc}} = \delta_{ck}A^{[l](m)}_{i^{'}j^{'}d},
\end{equation*}
and so
\begin{equation*}
  \frac{\uderivative \mathscr{L}}{\uderivative W^{[l]}_{dc}} = \sum_{m,i,j,k=1}^{M,H,W,C} \delta_{ck}A^{[l](m)}_{i^{'}j^{'}d} \frac{\partial \mathscr{L}}{\partial Z^{[l](m)}_{ijk}} = \sum_{m,i,j=1}^{M,H,W} A^{[l](m)}_{i^{'}j^{'}d} \frac{\partial \mathscr{L}}{\partial Z^{[l](m)}_{ijc}},
\end{equation*}
Q.E.D.

Similarly,
\begin{equation*}
  \frac{\uderivative \mathscr{L}}{\uderivative b^{[l]}_{c}} = \sum_{m,i,j,k=1}^{M,H,W,C} \frac{\partial \mathscr{L}}{\partial Z^{[l](m)}_{ijk}} \frac{\uderivative Z^{[l](m)}_{ijk}}{\uderivative b^{[l]}_{c}}.
\end{equation*}
Now,
\begin{equation*}
  \frac{\uderivative Z^{[l](m)}_{ijk}}{\uderivative b^{[l]}_{c}} = \delta_{ck},
\end{equation*}
and so
\begin{equation*}
  \frac{\uderivative \mathscr{L}}{\uderivative b^{[l]}_{c}} = \sum_{m,i,j,k=1}^{M,H,W,C} \delta_{ck} \frac{\partial \mathscr{L}}{\partial Z^{[l](m)}_{ijk}} = \sum_{m,i,j=1}^{M,H,W} \frac{\partial \mathscr{L}}{\partial Z^{[l](m)}_{ijc}},
\end{equation*}
Q.E.D.

Lastly,
\begin{equation*}
  \frac{\uderivative \mathscr{L}}{\uderivative A^{[l-1](m)}_{h^{'}w^{'}d}} = \sum_{n,i,j,k=1}^{M,H,W,C} \frac{\partial \mathscr{L}}{\partial Z^{[l](n)}_{ijk}} \frac{\uderivative Z^{[l](n)}_{ijk}}{\uderivative A^{[l-1](m)}_{h^{'}w^{'}d}}.
\end{equation*}
However,
\begin{equation*}
  \frac{\uderivative Z^{[l](n)}_{ijk}}{\uderivative A^{[l-1](m)}_{h^{'}w^{'}d}} = \delta_{mn} W^{[l]}_{kd},
\end{equation*}
and so
\begin{equation*}
  \frac{\uderivative \mathscr{L}}{\uderivative A^{[l-1](m)}_{h^{'}w^{'}d}} = \sum_{n,i,j,k=1}^{M,H,W,C} \delta_{mn}  W^{[l]}_{kd} \frac{\partial \mathscr{L}}{\partial Z^{[l](n)}_{ijk}} = \sum_{i,j,k=1}^{H,W,C} W^{[l]}_{kd} \frac{\partial \mathscr{L}}{\partial Z^{[l](m)}_{ijk}},
\end{equation*}
Q.E.D.

In all three cases,
\begin{equation*}
  \frac{\partial \mathscr{L}}{\partial Z^{[l](m)}_{ijk}} = \frac{\partial \mathscr{L}}{\partial A^{[l](m)}_{ijk}} \frac{\uderivative A^{[l](m)}_{ijk}}{\uderivative Z^{[l](m)}_{ijk}} = \frac{\partial \mathscr{L}}{\partial A^{[l](m)}_{ijk}} \frac{\uderivative g^{[l]}(Z^{[l](m)}_{ijk})}{\uderivative Z^{[l](m)}_{ijk}}.
\end{equation*}

\section{Pooling Layers} \label{sec:pool}

\appendix

%% The reference list follows the main body and any appendices.
%% Use LaTeX's thebibliography environment to mark up your reference list.
%% Note \begin{thebibliography} is followed by an empty set of
%% curly braces.  If you forget this, LaTeX will generate the error
%% "Perhaps a missing \item?".
%%
%% thebibliography produces citations in the text using \bibitem-\cite
%% cross-referencing. Each reference is preceded by a
%% \bibitem command that defines in curly braces the KEY that corresponds
%% to the KEY in the \cite commands (see the first section above).
%% Make sure that you provide a unique KEY for every \bibitem or else the
%% paper will not LaTeX. The square brackets should contain
%% the citation text that LaTeX will insert in
%% place of the \cite commands.

%% We have used macros to produce journal name abbreviations.
%% \aastex provides a number of these for the more frequently-cited journals.
%% See the Author Guide for a list of them.

%% Note that the style of the \bibitem labels (in []) is slightly
%% different from previous examples.  The natbib system solves a host
%% of citation expression problems, but it is necessary to clearly
%% delimit the year from the author name used in the citation.
%% See the natbib documentation for more details and options.

\begin{thebibliography}{}

\end{thebibliography}

%% This command is needed to show the entire author+affilation list when
%% the collaboration and author truncation commands are used.  It has to
%% go at the end of the manuscript.
%\allauthors

%% Include this line if you are using the \added, \replaced, \deleted
%% commands to see a summary list of all changes at the end of the article.
%\listofchanges

\end{document}

% End of file `sample61.tex'.
